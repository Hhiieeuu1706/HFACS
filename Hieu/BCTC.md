Abstract
Aviation accident investigation is a core obligation for operators because it enables the prevention of future occurrences. However, current investigations remain largely traditional and time-intensive, which delays assessment and safety improvements. Building on prior work in accident and human-factors analysis, this study proposes an automated pipeline that leverages large language models (LLMs) to code aviation accidents under the Human Factors Analysis and Classification System (HFACS). To address single-model bias and the need for explainability, we design a multi-agent architecture—an HFACS AI Expert Council—in which technical and organizational specialists analyze the same evidence, debate, and converge on a decision. The council’s qualitative rationales are then scored with a transparent rubric to ensure consistent and auditable outcomes. To evaluate the system rigorously, we develop a high-fidelity data simulator capable of generating diverse accident scenarios for synthesis and cross-checking. On a 100-run simulated dataset, the system attains an overall F1-score of 0.82, with strong performance on Level 3 (Unsafe Supervision) (F1 = 0.89) and Level 2 (Preconditions) (F1 = 0.85), and promising progress on Level 4 (Organizational Influences) (F1 = 0.54). These results indicate the feasibility of a reliable, explainable AI assistant that can accelerate analysis and support improvements in aviation safety.
Key words: AI, ChatGPT, HFACS, LLM.
Introduction
General aviation (GA) is a cornerstone of civil aviation, enabling missions that scheduled air transport cannot readily provide—humanitarian relief, emergency medical evacuation, environmental monitoring, and flexible cargo/personnel movement. Despite this central role, GA has faced persistent safety challenges for decades. At the investigative level, the NTSB indicates that a standard investigation typically spans 12–24 months [1], depending on case complexity and workload, which heightens pressure to reduce analytic latency and to standardize information flows. Within this context, the Human Factors Analysis and Classification System (HFACS) remains the primary taxonomy for human-factors analysis, encompassing four levels: Level 1 —Unsafe Acts, Level 2 —Preconditions for Unsafe Acts, Level 3 —Unsafe Supervision, and Level 4 —Organizational Influences. However, manually coding large volumes of narratives (accident reports, witness statements, CVR excerpts, maintenance records) is time-consuming and prone to inter-rater inconsistency.
 
Figure 1: The Swiss Cheese Model
With the rapid progress of large language models (LLMs), it is now feasible to standardize and streamline aviation-safety analytics—from entity extraction and topic classification to summarization and rapid reporting—outperforming many traditional workflows and improving consistency across raters [2-4]. Early applications of LLMs to accident analysis suggest shorter investigative cycles and reduced subjectivity when a clear taxonomy and scoring protocol guide judgments.
In this study, we present an automated pipeline for HFACS-based coding of aviation accidents designed with three constraints: explainability, inter-run consistency, and low latency for near-real-time use. At its core is a multi-agent architecture (HFACS AI Expert Council) in which role-specialized agents reason in parallel; their rationales are reconciled by a referee and converted to auditable, level-specific decisions via a rubric-based scorer. We evaluate the system on nine high-fidelity scenarios spanning all four HFACS levels over 100 controlled runs. The micro-averaged results are Precision = 0.58, Recall = 1.00, and F1 = 0.73, indicating a recall-oriented operating point suitable for triage while highlighting the need to raise precision (e.g., tighter thresholds and evidence grounding) before decision-grade deployment.
 
Figure 2: The HFACS (Human Factors Analysis and Classification System)
Related Work
Before LLMs, aviation-safety analytics largely relied on classical machine-learning pipelines—SVM, ANN/Decision Trees, and later CNN/RNN architectures—to classify occurrences and predict severity in civil-aviation datasets [5]. These models were useful but required extensive manually labeled corpora and struggled with the domain’s dense technical context and abbreviations, limiting generalization across operators and report styles.
Modern large language models (LLMs) have shifted this landscape by enabling robust reasoning over unstructured narratives with zero-/few-shot supervision. Early studies report gains in information extraction and classification from safety reports (e.g., GPT-4 on construction accident narratives) and in transportation incident analysis more broadly, suggesting shorter analytic cycles and improved coding consistency [6]. Yet most efforts stop at generic prompting and do not operationalize deep, HFACS-aligned causal reasoning.
Addressing that gap, Liu et al. (2025) introduce HFACS-guided Chain-of-Thought (HFACS-CoT) and an enhanced HFACS-CoT+, which embeds domain knowledge into stepwise prompts to structure the model’s reasoning and reduce logical slips [7]. In parallel, the literature on CoT with LLMs has expanded, offering broader evidence on how staged reasoning affects fidelity, error modes, and stability across tasks[8-10] .
Taken together, prior approaches either (i) depend on manual labels and lack domain semantics (traditional NLP/ML), (ii) use domain LMs/NER but do not integrate the HFACS taxonomy into the final reasoning step, or (iii) apply LLMs directly without controls for bias/consistency and without latency accounting for near-real-time use. Our work builds on these foundations and advances the state of practice by: (1) deploying a debate-style multi-agent architecture with an arbiter to mitigate single-model bias and improve explainability; (2) fusing text and telemetry so technical signals can be linked to human-factor findings; and (3) converting qualitative rationales into auditable, quantitative decisions via a rubric (HFACS_RUBRIC).
Methodology
Set up the framework
Figure 3.1 outlines a four-stage pipeline—Inputs → Simulate → Analyze → Outputs. A run is launched from the dashboard with a versioned scenario pack (JSON) and prompt; the simulator co-generates synchronized flight telemetry and narratives/maintenance notes. The analysis stage first screens for anomalies, then (only when triggered) convenes a multi-agent HFACS council whose outputs are reconciled by a referee and scored via a transparent HFACS rubric to yield level/tag decisions. Results are materialized as charts (PNG) and structured reports (CSV/JSON), while every run logs model version, inference parameters, seed, and timestamps to ensure reproducibility and end-to-end latency accounting—one traceable path from evidence to judgment with low overhead and clear avenues for scaling.
 
Figure 3.1 The structure of the research
Data Sampling

Model Performance Evaluation
This study adopts three standard classification metrics—Precision (P), Recall (R), and F1-score (F)—as summarized in Table 1. Precision is the proportion of correct positive detections among all positives flagged by the model, reflecting its ability to limit false positives (FP) in accident coding. Recall factor is the proportion of true positives recovered among all actual positives, capturing the model’s ability to avoid False negatives (FN) of human-factor findings. F1 is the harmonic mean of Precision and Recall, balancing the dual objectives of “no false alarms” and “no misses.” In the aviation-accident setting—where HFACS labels are often imbalanced and some categories are rare—F1 provides a robust single-number summary of performance.
Table 1: The Formula 
Metrics	Formular	Evaluation Focus
Precision (P)	TP/(TP+FP)	Correctly predicted positives in a positive class
Recall (R)	TP/(TP+FN)	Fraction of positive patterns correctly classified
F1-score (F)	(2*Precision*Recall)/(Precision+Recall)	Weighted average score of precision and recall
Results and Discussion
The Figure below summarizes the AI system’s HFACS classification results for 11 simulated general aviation accident scenarios.
 
Figure 4.1 The AI system’s HFACS classification of aviation accidents
The results demonstrate that the AI system can successfully map the major simulated incidents to the correct HFACS tier, particularly for Unsafe Supervision (Level 3) and Organizational Influences (Level 4) factors. However, the misclassifications highlight challenges in differentiating Preconditions for Unsafe Acts (Level 2) from both higher and lower levels. The model’s bias toward Level 3 classifications suggests that additional refinement is needed to improve discrimination between direct pilot errors, underlying preconditions, and supervisory vs. organizational factors in complex accident scenarios.
  
Figure 4.2 Overall HFACS Analysis Performance
Based on the data of 100 simulation runs as shown in Figure 4.2, the HFACS-coding system achieved Precision = 0.58, Recall = 1.00, and F1 = 0.73 (micro-average). This recall-saturated setting (FN ≈ 0) ensures comprehensive detection of human-factor findings but incurs a non-trivial false-alarm rate that depresses precision. In its current form, the model is appropriate for near-real-time triage—minimizing missed safety-critical signals—yet outputs should not be accepted without investigator review. Deployment should prioritize raising precision (tighter decision thresholds and rubric criteria, corroboration with telemetry and maintenance records) while maintaining recall; doing so should increase F1 and improve operational reliability. Overall, the configuration demonstrates strong coverage for HFACS analysis, but precision must be further optimized before use in decision-grade workflows.
 
Figure 4.3 HFACS Level-wise Performance Radar Chart
Across 200 simulation runs (Figure 4.2), the HFACS-coding system achieved P = 0.58, R = 1.00, and F1 = 0.73 (micro-average). This recall-saturated setting FN near value 0 ensures comprehensive detection of human-factor findings but incurs a non-trivial false-alarm rate that depresses precision. In its current form, the model is appropriate for near-real-time triage—minimizing missed safety-critical signals—yet outputs should not be accepted without investigator review. Deployment should prioritize raising precision (tighter decision thresholds and rubric criteria, corroboration with telemetry and maintenance records) while maintaining recall; doing so should increase F1 and improve operational reliability. Overall, the configuration demonstrates strong coverage for HFACS analysis, but precision must be further optimized before use in decision-grade workflows.
Hình 4.4 cho thấy rõ được được hiệu suất của hệ thống tùy thuộc vào sự cố. Trên tập 100 lần chạy theo từng kịch bản, mô hình vận hành ở điểm làm việc thiên về Recall: 10/11 kịch bản đạt R = 1.00, trong khi Precision dao động mạnh, vì vậy F1 phần lớn bị giới hạn bởi độ chính xác dự đoán. 
Nhìn vào kết quả đánh giá, sự biến động của F1 phụ thuộc lớn vào giá trị của Precision vì chỉ số R đều chiếm tỷ số tối ưu nhận khi gọi hệ thống. 
 
Figure 4.4 Performance of different scenarios
The error profile is concentrated in Level 1–2 tags; no Level 3 tag appears in the top-10, and only one Level 4 tag is present. Color segmentation indicates FP dominates many L1/L2 items, whereas FN clusters on cognitively demanding tags. 
 
Figure 4.5 Top 10 HFACS Tags with Most Errors

The two main contributors, L1_ATTENTION_FAILURES (55) and L1_RULE_BASED_DECISIONS (54), are primarily FP, though the latter also exhibits noteworthy FN. FP is also skewed by a second tier, L2_EQUIPMENT_AND_CONTROLS (35), L1_ILL_STRUCTURED_DECISIONS (28), L4_OPERATIONS_PROCESS (26), and L2_MENTAL_FATIGUE (26), which suggests an excessive sensitivity to behavioral/equipment cues. However, for L1_MISJUDGMENTS (31), L1_CHOICE_DECISIONS (28), L1_TECHNIQUE_ERRORS (26), and L2_INTERFACES_AND_DISPLAYS (26), which call for more complex cognitive/interface reasoning, FN is prominent. Overall, the pattern identifies two areas that need improvement: improving the recognition of cognitive/interface indicators to lower FN and tightening the evidence thresholds for L1 behavioral tags to curb FP.
Conclusion 
This study introduces an automated HFACS coding pipeline that combines a multi-agent architecture with an arbiter and a standardized rubric to deliver low-latency, near–real-time analysis of accident narratives. On a high-fidelity simulator spanning nine scenarios over 100 runs, the system attains F1 = 0.82 overall, with strong level-wise performance at Level 3 (Unsafe Supervision, F1 = 0.89) and Level 2 (Preconditions, F1 = 0.85). These results indicate that debate-style multi-agent reasoning, followed by rubric-based scoring, can mitigate single-model bias and improve consistency and explainability in large-scale human-factors coding. The main limitation is the simulation-to-field gap. Accordingly, future work will integrate retrieval-augmented grounding (RAG) from historical cases to support evidence-based decisions, replace the rule-based anomaly gate with a machine-learning anomaly detector trained on nominal flight data, and incorporate a human-in-the-loop interface for expert review and continual refinement on real-world reports.

Reference
[1]	H. Said, “2024 Full Year Accident Update.”
[2]	Q. Liu, F. Li, K. K. H. Ng, J. Han, and S. Feng, “Accident investigation via LLMs reasoning: HFACS-guided Chain-of-Thoughts enhance general aviation safety,” Expert Syst Appl, vol. 269, Apr. 2025, doi: 10.1016/j.eswa.2025.126422.
[3]	C. Chandra, Y. Ojima, M. V. Bendarkar, and D. N. Mavris, “Aviation-BERT-NER: Named Entity Recognition for Aviation Safety Reports,” Aerospace, vol. 11, no. 11, Nov. 2024, doi: 10.3390/aerospace11110890.
[4]	S. R. Andrade and H. S. Walsh, “SafeAeroBERT: Towards a Safety-Informed Aerospace-Specific Language Model A growing body of research has demonstrated the effectiveness of pre-training Bidirectional Encoder Representations from Transformers (BERT) models [1] on domain-specific text heavy with jargon. BERT models.”
[5]	X. Zhang and S. Mahadevan, “Bayesian network modeling of accident investigation reports for aviation safety assessment,” Reliab Eng Syst Saf, vol. 209, 2021, doi: 10.1016/j.ress.2020.107371.
[6]	E. Ahmadi, S. Muley, and C. Wang, “Automatic Construction Accident Report Analysis Using Large Language Models (LLMs),” Journal of Intelligent Construction, vol. 3, no. 1, pp. 1–10, Jun. 2024, doi: 10.26599/jic.2024.9180039.
[7]	F. Wang, A. Liu, C. Qu, R. Xiong, and L. Chen, “A Deep-Learning Method for Remaining Useful Life Prediction of Power Machinery via Dual-Attention Mechanism,” Sensors, vol. 25, no. 2, Jan. 2025, doi: 10.3390/s25020497.
[8]	E. Ahmadi, S. Muley, C. Wang, S. S. Bert Turner, and P. S. Advisor Bert Turner, “Automatic Construction Accident Report Analysis Using Large Language Models.” [Online]. Available: https://ssrn.com/abstract=4735131
[9]	S. V. T. Tran et al., “LEVERAGING LARGE LANGUAGE MODELS FOR ENHANCED CONSTRUCTION SAFETY REGULATION EXTRACTION,” Journal of Information Technology in Construction, vol. 29, pp. 1026–1038, 2024, doi: 10.36680/j.itcon.2024.045.
[10]	H. Zhen, Y. Shi, Y. Huang, J. J. Yang, and N. Liu, “Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering for Traffic Crash Severity Analysis and Inference,” Aug. 2024, [Online]. Available: http://arxiv.org/abs/2408.04652
 
